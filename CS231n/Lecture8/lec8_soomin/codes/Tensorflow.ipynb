{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''V1'''\n",
    "import numpy as np\n",
    "import tensrotflow as tf\n",
    "\n",
    "# 1. define computational graph\n",
    "N, D, H = 64, 1000, 100\n",
    "x = tf.placeholder(tf.float32, shape=(N, D)) # create placeholders\n",
    "y = tf.placeholder(tf.float32, shape=(N, D))\n",
    "w1 = tf.placeholder(tf.float32, shape=(D, H)) \n",
    "w2 = tf.placeholder(tf.float32, shape=(H, D))\n",
    "\n",
    "# forward pass / no computation. just build.\n",
    "h = tf.maximum(tf.matmul(x, w1), 0)\n",
    "y_pred = tf.matmul(h, w2)\n",
    "diff = y_pred - y\n",
    "loss = tf.reduce_mean(tf.reduce_sum(diff**2, axis=1)) # L2 dist(y, y_pred)\n",
    "\n",
    "# loss of gradient / no computation. just build.\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w1])\n",
    "\n",
    "# 2. run the graph many times with feeding data\n",
    "with tf.Session() as sess:\n",
    "    values = {  x: np.random.randn(N, D), # numpy arrays to fill placeholders\n",
    "                w1: np.random.randn(D, H),\n",
    "                w2: np.random.randn(H, D),\n",
    "                y: np.random.randn(N, D), }\n",
    "    learning_rate = 1e-5\n",
    "    # train the network\n",
    "    for t in range(50):\n",
    "        out = sess.run([loss, grad_w1, grad_w2], feed_dict=values) # run the graph\n",
    "        loss_val, grad_w1_val, grad_w2_val = out # output: arrays\n",
    "\n",
    "        values[w1] -= learning_rate * grad_w1_val # use gradient to update weights\n",
    "        values[w2] -= learning_rate * grad_w2_val\n",
    "\n",
    "'''\n",
    "Problem: copying weights between CPU & GPU each step\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''V2'''\n",
    "import numpy as np\n",
    "import tensrotflow as tf\n",
    "\n",
    "# 1. define computational graph\n",
    "N, D, H = 64, 1000, 100\n",
    "x = tf.placeholder(tf.float32, shape=(N, D)) # create placeholders\n",
    "y = tf.placeholder(tf.float32, shape=(N, D))\n",
    "''''''\n",
    "w1 = tf.Variable(tf.float32, shape=(D, H)) # create Variables\n",
    "w2 = tf.Variable(tf.float32, shape=(H, D))\n",
    "\n",
    "# forward pass / no computation. just build.\n",
    "h = tf.maximum(tf.matmul(x, w1), 0)\n",
    "y_pred = tf.matmul(h, w2)\n",
    "diff = y_pred - y\n",
    "loss = tf.reduce_mean(tf.reduce_sum(diff**2, axis=1)) # L2 dist(y, y_pred)\n",
    "\n",
    "# loss of gradient / no computation. just build.\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w1])\n",
    "\n",
    "''''''\n",
    "learning_rate = 1e-5\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "\n",
    "# 2. run the graph many times with feeding data\n",
    "with tf.Session() as sess:\n",
    "    ''''''\n",
    "    sess.run(tf.global_variables_initializer()) # run graph once to initialize w1, w2\n",
    "    values = {  x: np.random.randn(N, D), # numpy arrays to fill placeholders\n",
    "                y: np.random.randn(N, D), }\n",
    "\n",
    "    ''''''\n",
    "    # train the network\n",
    "    for t in range(50):\n",
    "        loss_val, = sess.run([loss], feed_dict=values) # run the graph\n",
    "        \n",
    "\n",
    "'''\n",
    "Problem: loss not going down\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''V3'''\n",
    "import numpy as np\n",
    "import tensrotflow as tf\n",
    "\n",
    "# 1. define computational graph\n",
    "N, D, H = 64, 1000, 100\n",
    "x = tf.placeholder(tf.float32, shape=(N, D)) # create placeholders\n",
    "y = tf.placeholder(tf.float32, shape=(N, D))\n",
    "w1 = tf.Variable(tf.float32, shape=(D, H)) # create Variables\n",
    "w2 = tf.Variable(tf.float32, shape=(H, D))\n",
    "\n",
    "# forward pass / no computation. just build.\n",
    "h = tf.maximum(tf.matmul(x, w1), 0)\n",
    "y_pred = tf.matmul(h, w2)\n",
    "diff = y_pred - y\n",
    "loss = tf.reduce_mean(tf.reduce_sum(diff**2, axis=1)) # L2 dist(y, y_pred)\n",
    "\n",
    "# loss of gradient / no computation. just build.\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w1])\n",
    "\n",
    "learning_rate = 1e-5\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "\n",
    "''''''\n",
    "updates = tf.group(new_w1, new_w2) # add dummy graph node\n",
    "\n",
    "# 2. run the graph many times with feeding data\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) # run graph once to initialize w1, w2\n",
    "    values = {  x: np.random.randn(N, D), # numpy arrays to fill placeholders\n",
    "                y: np.random.randn(N, D), }\n",
    "\n",
    "    ''''''\n",
    "    # train the network\n",
    "    for t in range(50):\n",
    "        loss_val, _ = sess.run([loss, updates], feed_dict=values) # run the graph & compute dummy node(null return)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''original'''\n",
    "import numpy as np\n",
    "import tensrotflow as tf\n",
    "\n",
    "# 1. define computational graph\n",
    "N, D, H = 64, 1000, 100\n",
    "x = tf.placeholder(tf.float32, shape=(N, D)) \n",
    "y = tf.placeholder(tf.float32, shape=(N, D))\n",
    "w1 = tf.Variable(tf.float32, shape=(D, H))\n",
    "w2 = tf.Variable(tf.float32, shape=(H, D))\n",
    "\n",
    "# forward pass / no computation. just build.\n",
    "h = tf.maximum(tf.matmul(x, w1), 0)\n",
    "y_pred = tf.matmul(h, w2)\n",
    "diff = y_pred - y\n",
    "loss = tf.reduce_mean(tf.reduce_sum(diff**2, axis=1))\n",
    "\n",
    "# loss of gradient / no computation. just build.\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w1])\n",
    "\n",
    "learning_rate = 1e-5\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "\n",
    "''''''\n",
    "updates = tf.group(new_w1, new_w2) # add dummy graph node\n",
    "\n",
    "# 2. run the graph many times with feeding data\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) # run graph once to initialize w1, w2\n",
    "    values = {  x: np.random.randn(N, D), # numpy arrays to fill placeholders\n",
    "                y: np.random.randn(N, D), }\n",
    "\n",
    "    ''''''\n",
    "    # train the network\n",
    "    for t in range(50):\n",
    "        loss_val, _ = sess.run([loss, updates], feed_dict=values) # run the graph & compute dummy node(null return)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''optimization'''\n",
    "import numpy as np\n",
    "import tensrotflow as tf\n",
    "\n",
    "# 1. define computational graph\n",
    "N, D, H = 64, 1000, 100\n",
    "x = tf.placeholder(tf.float32, shape=(N, D))\n",
    "y = tf.placeholder(tf.float32, shape=(N, D))\n",
    "w1 = tf.Variable(tf.float32, shape=(D, H))\n",
    "w2 = tf.Variable(tf.float32, shape=(H, D))\n",
    "\n",
    "# forward pass / no computation. just build.\n",
    "h = tf.maximum(tf.matmul(x, w1), 0)\n",
    "y_pred = tf.matmul(h, w2)\n",
    "diff = y_pred - y\n",
    "loss = tf.reduce_mean(tf.reduce_sum(diff**2, axis=1)) \n",
    "\n",
    "''''''\n",
    "optimizer = tf.train.GradientDescentOprimizer(1e-5) # optimizer: compute grad & update W\n",
    "updates = optimizer.minimize(loss)\n",
    "\n",
    "# 2. run the graph many times with feeding data\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) # run graph once to initialize w1, w2\n",
    "    values = {  x: np.random.randn(N, D), # numpy arrays to fill placeholders\n",
    "                y: np.random.randn(N, D), }\n",
    "\n",
    "    losses = []\n",
    "    # train the network\n",
    "    for t in range(50):\n",
    "        loss_val, _ = sess.run([loss, updates], feed_dict=values) # run the graph & exec optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loss'''\n",
    "import numpy as np\n",
    "import tensrotflow as tf\n",
    "\n",
    "# 1. define computational graph\n",
    "N, D, H = 64, 1000, 100\n",
    "x = tf.placeholder(tf.float32, shape=(N, D))\n",
    "y = tf.placeholder(tf.float32, shape=(N, D))\n",
    "w1 = tf.Variable(tf.float32, shape=(D, H))\n",
    "w2 = tf.Variable(tf.float32, shape=(H, D))\n",
    "\n",
    "# forward pass / no computation. just build.\n",
    "h = tf.maximum(tf.matmul(x, w1), 0)\n",
    "y_pred = tf.matmul(h, w2)\n",
    "''''''\n",
    "loss = tf.losses.mean_squared_error(y_pred, y) # predefined loss\n",
    "\n",
    "optimizer = tf.train.GradientDescentOprimizer(1e-3) # optimizer: compute grad & update W\n",
    "updates = optimizer.minimize(loss)\n",
    "\n",
    "# 2. run the graph many times with feeding data\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) # run graph once to initialize w1, w2\n",
    "    values = {  x: np.random.randn(N, D), # numpy arrays to fill placeholders\n",
    "                y: np.random.randn(N, D), }\n",
    "\n",
    "    losses = []\n",
    "    # train the network\n",
    "    for t in range(50):\n",
    "        loss_val, _ = sess.run([loss, updates], feed_dict=values) # run the graph & exec optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Layers'''\n",
    "import numpy as np\n",
    "import tensrotflow as tf\n",
    "\n",
    "# 1. define computational graph\n",
    "N, D, H = 64, 1000, 100\n",
    "x = tf.placeholder(tf.float32, shape=(N, D))\n",
    "y = tf.placeholder(tf.float32, shape=(N, D))\n",
    "\n",
    "''''''\n",
    "# automatically set up W (tf.layers)\n",
    "init = tf.contrib.layers.xavier_initializer() # Xavier initializer\n",
    "h = tf.layers.dense(inputs=x, units=H, activation=tf.nn.relu, kernel_initializer=init)\n",
    "y_pred = tf.layers.dense(inputs=h, units=D, kernel_initializer=init)\n",
    "\n",
    "# forward pass / no computation. just build.\n",
    "h = tf.maximum(tf.matmul(x, w1), 0)\n",
    "y_pred = tf.matmul(h, w2)\n",
    "loss = tf.losses.mean_squared_error(y_pred, y) # predefined loss\n",
    "\n",
    "optimizer = tf.train.GradientDescentOprimizer(1e-3) # optimizer: compute grad & update W\n",
    "updates = optimizer.minimize(loss)\n",
    "\n",
    "# 2. run the graph many times with feeding data\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) # run graph once to initialize w1, w2\n",
    "    values = {  x: np.random.randn(N, D), # numpy arrays to fill placeholders\n",
    "                y: np.random.randn(N, D), }\n",
    "\n",
    "    losses = []\n",
    "    # train the network\n",
    "    for t in range(50):\n",
    "        loss_val, _ = sess.run([loss, updates], feed_dict=values) # run the graph & exec optimizer\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
